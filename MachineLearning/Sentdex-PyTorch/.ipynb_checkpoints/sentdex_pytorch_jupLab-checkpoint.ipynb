{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23a17305-dcd9-4b75-833d-38f0f146db60",
   "metadata": {
    "id": "qwOuabTMCl5Z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (1.10.0)\n",
      "Requirement already satisfied: torchvision in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (0.11.1)\n",
      "Requirement already satisfied: torchaudio in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (0.10.0)\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from torch) (3.10.0.0)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from torchvision) (1.21.4)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from torchvision) (8.2.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.7/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio\n",
    "import torch\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d24db35-ffcc-48bc-84e0-a01851c79f7b",
   "metadata": {
    "id": "iCrdXfBLCwEf"
   },
   "outputs": [],
   "source": [
    "x = torch.Tensor([5, 3])\n",
    "y = torch.Tensor([2, 1])\n",
    "\n",
    "x = torch.zeros([2, 5])\n",
    "y = torch.rand([2, 5])\n",
    "\n",
    "# Changing the shape of the Tensor\n",
    "y = y.view([1, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6dd33ff-747b-4431-b7ac-aedf4af64c88",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443,
     "referenced_widgets": [
      "ec7b680efcc84ccfa6f669fafa12d2e5",
      "d65146cc7bb4402ba4fd4353aca46ab1",
      "9987e2a3e2a641b7a5173982e2b40b7d",
      "5bb4218c86934c449d22883ef7dc9f7d",
      "012c4097fe474f0e865f8eecbb6242bd",
      "48d3ff37676942409d1d9f5ab48a762a",
      "537519675989406ea809ec29f63edcb9",
      "7ae73644ef5c44bbbaf4f9c535c64993",
      "0705053bf1ef49bda6620c3cb1635bfc",
      "fce54ab02c844e83ad7228b5fb13c9a9",
      "c7f189e69c41487d8128f431a31f8176",
      "7756405d0c114a479d87c20cba6d6b0d",
      "e95342f4dd8a449385766e008819faf0",
      "516542303e854da2ad0a03d1017245ff",
      "aba6cba6372b420b888ba00a2135869d",
      "b72e25574c7f4c988bcf3e09b3a8882e",
      "d9ec27228d5145a3ad1b30370f57a9dc",
      "19bf972b752541cfbb4732483e8488c6",
      "e40802c6f8a344a6afbdf770c2c6f333",
      "6569105cc1984ce5bee8187b25d9c4f9",
      "cd5805bbce6d4fac820b1c3a98d7acd9",
      "ab761794d8a1429d8af4c7cf40537079",
      "14a76f7e77aa4f74be4dda8ca34e990d",
      "dafebcc64f5b4476a58706c8c0546a8c",
      "c2fa7accefba4be69848091b3e0cfa48",
      "47c5eb2c472e4a70a12273204d2770f8",
      "a33a925f36ef42b88be679d029e5daee",
      "eb796b1209244043ba224eb72ba4c0dc",
      "1b9ba0b385fb403d9ddca3fc5a878a7d",
      "e5d65798bc514ab0958a9e4c6f9440e5",
      "d68486e8185847328671b670bebbe5cb",
      "018d5bc08866419caa222d7576c0b40e",
      "22bb5025519546fe887a0ced405e2419",
      "c67e1e9f51f74e8085d657538c748ce3",
      "8e657a099c55454abd7980db973c8576",
      "08181a07e61c4bc482ec5bc45863b95c",
      "5deede5e681e47469eef3b8147e78910",
      "c2240846d79c4983ba34af747923c1b2",
      "62add7ce4e61492b8d6fb3cb5ef6a66d",
      "16e60050d1bb472abdb210d0b678c0ed",
      "d9dca9aefe2c4ea3b3a0428b82265fc3",
      "1f307c6b63924ee78e1a52cff1c01ae5",
      "2e110e8344c54584bb4a16b773640eea",
      "f55ba9e29dc04fc1b808e05b5b1c0bbc"
     ]
    },
    "id": "s8nuT0EXEpb6",
    "outputId": "6fdeb85c-0089-4785-9af3-7201be669edc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.2%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102.8%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST/raw/train-labels-idx1-ubyte.gz to MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "78.9%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112.7%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST/raw/t10k-labels-idx1-ubyte.gz to MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Pass in transformers to the data here\n",
    "train = datasets.MNIST(\"\", train=True, download=True,\n",
    "                       transform = transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "test = datasets.MNIST(\"\", train=False, download=True,\n",
    "                       transform = transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6ee1205-face-4b53-a89a-9975f278549a",
   "metadata": {
    "id": "KVyb3jNnKmwY"
   },
   "outputs": [],
   "source": [
    "# Using batches of data is more efficient, especially when the dataset\n",
    "# is huge. Also, passing in batches lets the machine learn more generalizations\n",
    "trainset = torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)\n",
    "testset = torch.utils.data.DataLoader(test, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76f747c1-548f-4e3f-8fba-e9c96c45beaf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sz42WEIuMsOO",
    "outputId": "d51b89cf-431e-4e1e-c7ca-06e74d14493c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([8, 5, 8, 6, 3, 6, 9, 6, 9, 3])]\n"
     ]
    }
   ],
   "source": [
    "for data in trainset:\n",
    "  print(data)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6749c51-a4b1-4395-b2b7-c0b130239627",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9_ZD49lBNEdn",
    "outputId": "d1cc88b8-3efb-4ea3-c695-45e3c4c61092"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8)\n"
     ]
    }
   ],
   "source": [
    "x, y = data[0][0], data[1][0]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b79da5f-c983-40cd-84de-8f78eb47f24a",
   "metadata": {
    "id": "9LYxFt9aNdWs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.5.0-cp37-cp37m-macosx_10_9_x86_64.whl (7.3 MB)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Using cached kiwisolver-1.3.2-cp37-cp37m-macosx_10_9_x86_64.whl (61 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from matplotlib) (20.9)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.28.2-py3-none-any.whl (880 kB)\n",
      "\u001b[K     |████████████████████████████████| 880 kB 2.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.2.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from matplotlib) (2.4.7)\n",
      "Collecting setuptools-scm>=4\n",
      "  Using cached setuptools_scm-6.3.2-py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from matplotlib) (1.21.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from matplotlib) (2.8.1)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from matplotlib) (8.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib) (1.14.0)\n",
      "Collecting tomli>=1.0.0\n",
      "  Using cached tomli-1.2.2-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from setuptools-scm>=4->matplotlib) (41.2.0)\n",
      "Installing collected packages: tomli, setuptools-scm, kiwisolver, fonttools, cycler, matplotlib\n",
      "Successfully installed cycler-0.11.0 fonttools-4.28.2 kiwisolver-1.3.2 matplotlib-3.5.0 setuptools-scm-6.3.2 tomli-1.2.2\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.7/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51bba036-3dc4-4786-a885-58b126cdae00",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "TLOn3zUUNg7v",
    "outputId": "758148e2-ce30-4453-b65e-da0d9301dc45"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PyTorch has a different shaping standard. There's a 1x28x28 shaping\n",
    "# style for an image.\n",
    "\n",
    "plt.imshow(x.view([28, 28]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab2e33d8-7785-4043-bc79-68266dee5ed9",
   "metadata": {
    "id": "VZgYp1xiOX_g"
   },
   "outputs": [],
   "source": [
    "# Optimizer tries to decrease loss and it doesn't have any idea\n",
    "# how much better we could get. You want to balance the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8bc10d6-b4b4-4856-82e5-1d1cfa51a660",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xEvNdJe5O4zN",
    "outputId": "166001b4-aab8-4cc7-a45d-54d4bb1db25c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 5923, 1: 6742, 2: 5958, 3: 6131, 4: 5842, 5: 5421, 6: 5918, 7: 6265, 8: 5851, 9: 5949}\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "counter_dict = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0, 9:0}\n",
    "for data in trainset:\n",
    "  xs, ys = data\n",
    "  \n",
    "  for y in ys:\n",
    "    counter_dict[int(y)] += 1\n",
    "    total += 1\n",
    "\n",
    "print(counter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62c233b7-d635-4145-af1b-bcdcaec6f7ad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "piAEpv1_Pprl",
    "outputId": "fc84e926-bc77-4760-bdb4-c8cf6ed03e6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 9.871666666666666\n",
      "1: 11.236666666666666\n",
      "2: 9.93\n",
      "3: 10.218333333333334\n",
      "4: 9.736666666666666\n",
      "5: 9.035\n",
      "6: 9.863333333333333\n",
      "7: 10.441666666666666\n",
      "8: 9.751666666666667\n",
      "9: 9.915000000000001\n"
     ]
    }
   ],
   "source": [
    "for i in counter_dict:\n",
    "  print(f\"{i}: {counter_dict[i]/total*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30a26610-895e-4ced-844a-0710b9501442",
   "metadata": {
    "id": "w4wmtz98QNLu"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "45d9c952-363e-45b5-893c-ae5313d5a7ef",
   "metadata": {
    "id": "_6S2Qx5cQb8a"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    # 3 layers of 64 neurons for our hidden layer\n",
    "    self.fc1 = nn.Linear(784, 64) # Activation function runs on the output side\n",
    "    self.fc2 = nn.Linear(64, 64)\n",
    "    self.fc3 = nn.Linear(64, 64)\n",
    "    self.fc4 = nn.Linear(64, 10)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # Feed forward neural net\n",
    "    # The activation function is whether or not the neuron is firing\n",
    "    # Don't wanna run relu on the output layer. Log softmax is a good choice.\n",
    "\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = F.relu(self.fc2(x))\n",
    "    x = F.relu(self.fc3(x))\n",
    "    x = self.fc4(x)\n",
    "\n",
    "    # Which dimension to apply softmax on? Output is flat, so dim=1\n",
    "    # It is similar to axes.\n",
    "    return F.log_softmax(x, dim=1)\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0ace3ba-5ce1-4ed5-adfb-894bb4118531",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qhPkuFWzkOg5",
    "outputId": "05ef0dd7-63cd-4f1a-bf21-1fedb77d9471"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.3616, -2.2694, -2.3541, -2.1959, -2.2857, -2.2462, -2.4061, -2.3712,\n",
      "         -2.3307, -2.2269]], grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand((28, 28))\n",
    "# Why is the -1 here? Says that the input is of unknown shape\n",
    "X = X.view(-1, 28*28)\n",
    "output = net(X)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27736b28-2b70-426a-a7b0-f38ecb5021bc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "93qxQA4RdpOG",
    "outputId": "f8f00230-1e25-415b-e610-6c4b5c04bb9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1545, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3452, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0661, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Loss and Optimizer. We want to minimize loss obviously. \n",
    "# Note - You can tell your neural network which weights to adjust\n",
    "import torch.optim as optim\n",
    "\n",
    "# You don't want your learning rate to be too large as it won't\n",
    "# catch the nuances of the data. Also, it shouldn't be too small\n",
    "# as the solution will only be a local maxima/minima. \n",
    "\n",
    "# On complex problems, we use a decaying learning rate.\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "EPOCHS = 3 # 3 full passes through data\n",
    "\n",
    "# Zero gradient is if you wanna batch the data up\n",
    "# 2 main ways of calculating loss: one hot vector ([0, 1, 0, ...]) \n",
    "# and nll (for scalar data)\n",
    "for epoch in range(EPOCHS):\n",
    "  for data in trainset:\n",
    "    X, y = data\n",
    "    net.zero_grad()\n",
    "    output = net(X.view(-1, 28*28))\n",
    "    loss = F.nll_loss(output, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "  print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c493adc3-f4f6-4f19-ba75-85ac2e3cd840",
   "metadata": {
    "id": "h_mOBrKuiQjz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "  for data in trainset:\n",
    "    X, y = data\n",
    "    output = net(X.view(-1, 28*28))\n",
    "    for idx, i in enumerate(output):\n",
    "      if torch.argmax(i) == y[idx]:\n",
    "        correct += 1\n",
    "      \n",
    "      total += 1\n",
    "\n",
    "print(\"Accuracy: {}\".format(round(correct/total, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "128861b5-cee1-407e-a77f-57533195ea7c",
   "metadata": {
    "id": "I6dHXqrgDyJY"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f21939b2-4782-427b-995d-3163b8692b66",
   "metadata": {
    "id": "EeqRmKZ3Dzz1"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7)\n"
     ]
    }
   ],
   "source": [
    "plt.imshow(X[1].view(28, 28))\n",
    "plt.show()\n",
    "print(torch.argmax(net(X[1].view(-1, 28*28))[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3a4f6f-408d-4e18-902e-8d3195814d70",
   "metadata": {
    "id": "GxAB0FKjE0LN"
   },
   "source": [
    "# CNN (Convo Neural Net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cdd516be-f0b4-412f-856b-722c5f202743",
   "metadata": {
    "id": "Jfk83ZuYE8Sc"
   },
   "outputs": [],
   "source": [
    "# Traditionally, CNNs were used for image stuff. However, in recent\n",
    "# year, they have even beat RNNs (Reccurent) in sequential problems.\n",
    "# Images are arrays of pixels. Each convolution (or kernel) takes a\n",
    "# subset of these pixels and look for features within them.\n",
    "\n",
    "# Condense the image and then pool them (maxpooling). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b817ea4c-c952-43f6-b4b8-96a0088d15ab",
   "metadata": {
    "id": "OuTrfb80NlEa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (1.21.4)\n",
      "Requirement already satisfied: opencv-python in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (4.5.4.60)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n",
      "\u001b[K     |████████████████████████████████| 76 kB 5.3 MB/s eta 0:00:011\n",
      "\u001b[?25hInstalling collected packages: tqdm\n",
      "Successfully installed tqdm-4.62.3\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.7/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install numpy opencv-python tqdm\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm # progress bar        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7ea11ffe-2035-408d-8345-7fac85c3c94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|███████                              | 2383/12502 [00:04<00:20, 485.72it/s]Corrupt JPEG data: 214 extraneous bytes before marker 0xd9\n",
      " 69%|█████████████████████████▌           | 8658/12502 [00:15<00:07, 543.25it/s]Corrupt JPEG data: 1153 extraneous bytes before marker 0xd9\n",
      " 75%|███████████████████████████▋         | 9369/12502 [00:16<00:06, 484.03it/s]Corrupt JPEG data: 99 extraneous bytes before marker 0xd9\n",
      " 86%|██████████████████████████████▉     | 10742/12502 [00:19<00:03, 560.78it/s]Corrupt JPEG data: 128 extraneous bytes before marker 0xd9\n",
      " 90%|████████████████████████████████▍   | 11268/12502 [00:20<00:02, 581.05it/s]Corrupt JPEG data: 239 extraneous bytes before marker 0xd9\n",
      "100%|████████████████████████████████████| 12502/12502 [00:22<00:00, 554.33it/s]\n",
      " 10%|███▌                                 | 1204/12501 [00:02<00:19, 571.52it/s]Corrupt JPEG data: 65 extraneous bytes before marker 0xd9\n",
      " 39%|██████████████▌                      | 4921/12501 [00:09<00:15, 501.53it/s]Corrupt JPEG data: 226 extraneous bytes before marker 0xd9\n",
      " 56%|████████████████████▌                | 6941/12501 [00:13<00:12, 461.66it/s]Corrupt JPEG data: 162 extraneous bytes before marker 0xd9\n",
      " 60%|██████████████████████               | 7452/12501 [00:14<00:09, 534.88it/s]Warning: unknown JFIF revision number 0.00\n",
      " 75%|███████████████████████████▌         | 9317/12501 [00:18<00:06, 529.61it/s]Corrupt JPEG data: 2230 extraneous bytes before marker 0xd9\n",
      " 84%|██████████████████████████████▎     | 10512/12501 [00:20<00:03, 498.99it/s]Corrupt JPEG data: 254 extraneous bytes before marker 0xd9\n",
      " 91%|████████████████████████████████▊   | 11403/12501 [00:22<00:02, 524.94it/s]Corrupt JPEG data: 399 extraneous bytes before marker 0xd9\n",
      " 97%|██████████████████████████████████▊ | 12095/12501 [00:23<00:00, 530.62it/s]Corrupt JPEG data: 1403 extraneous bytes before marker 0xd9\n",
      "100%|████████████████████████████████████| 12501/12501 [00:24<00:00, 506.33it/s]\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/numpy/lib/npyio.py:528: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.asanyarray(arr)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cats: 12476\n",
      "Dogs: 12470\n"
     ]
    }
   ],
   "source": [
    "REBUILD_DATA = False\n",
    "\n",
    "class DogsVSCats:\n",
    "  # Images have varying size and shape, but we want them\n",
    "  # to be uniform.\n",
    "  IMG_SIZE = 50\n",
    "  CATS = \"catsVSdogs/PetImages/Cat\"\n",
    "  DOGS = \"catsVSdogs/PetImages/Dog\"\n",
    "  LABELS = {CATS: 0, DOGS: 1}\n",
    "\n",
    "  training_data = []\n",
    "  # MAKE SURE THE DATASET IS BALANCED. If there are 10000 pictures of cats and 100 of dogs, you're not\n",
    "  # gonna make a decent neural network ovviamente.\n",
    "  catcount = 0\n",
    "  dogcount = 0\n",
    " \n",
    "    # Everything is a feature to the machine, even the colors of\n",
    "    # an image. So think about whether color makes a difference\n",
    "    # to your neural net.\n",
    "  def make_training_data(self):\n",
    "    for label in self.LABELS:\n",
    "      for f in tqdm(os.listdir(label)):\n",
    "        try:\n",
    "            path = os.path.join(label, f)\n",
    "            img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "            img = cv2.resize(img, (self.IMG_SIZE, self.IMG_SIZE))\n",
    "\n",
    "            # One hot vectors allow a better loss metric. Once again,\n",
    "            # an example of a one hot vector would be [1, 0] if it's\n",
    "            # a cat and [0, 1] if it's a dog. The np.eye function\n",
    "            # helps doing that with the diagonals.\n",
    "            self.training_data.append([np.array(img), np.eye(2)[self.LABELS[label]]])\n",
    "\n",
    "            if label == self.CATS:\n",
    "                self.catcount += 1\n",
    "\n",
    "            elif label == self.DOGS:\n",
    "                self.dogcount += 1\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        \n",
    "    np.random.shuffle(self.training_data)\n",
    "    np.save(\"training_data.npy\", self.training_data)\n",
    "    print(\"Cats: {}\".format(self.catcount))\n",
    "    print(\"Dogs: {}\".format(self.dogcount))\n",
    "    \n",
    "\n",
    "if REBUILD_DATA:\n",
    "    dogvcats = DogsVSCats()\n",
    "    dogvcats.make_training_data()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867ec211-d1b5-4d3d-96d0-592821e235f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = np.load(\"training_data.npy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
