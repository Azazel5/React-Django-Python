{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23a17305-dcd9-4b75-833d-38f0f146db60",
   "metadata": {
    "id": "qwOuabTMCl5Z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (1.10.0)\n",
      "Requirement already satisfied: torchvision in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (0.11.1)\n",
      "Requirement already satisfied: torchaudio in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (0.10.0)\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from torch) (3.10.0.0)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from torchvision) (1.21.4)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from torchvision) (8.2.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.7/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio\n",
    "import torch\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d24db35-ffcc-48bc-84e0-a01851c79f7b",
   "metadata": {
    "id": "iCrdXfBLCwEf"
   },
   "outputs": [],
   "source": [
    "x = torch.Tensor([5, 3])\n",
    "y = torch.Tensor([2, 1])\n",
    "\n",
    "x = torch.zeros([2, 5])\n",
    "y = torch.rand([2, 5])\n",
    "\n",
    "# Changing the shape of the Tensor\n",
    "y = y.view([1, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6dd33ff-747b-4431-b7ac-aedf4af64c88",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443,
     "referenced_widgets": [
      "ec7b680efcc84ccfa6f669fafa12d2e5",
      "d65146cc7bb4402ba4fd4353aca46ab1",
      "9987e2a3e2a641b7a5173982e2b40b7d",
      "5bb4218c86934c449d22883ef7dc9f7d",
      "012c4097fe474f0e865f8eecbb6242bd",
      "48d3ff37676942409d1d9f5ab48a762a",
      "537519675989406ea809ec29f63edcb9",
      "7ae73644ef5c44bbbaf4f9c535c64993",
      "0705053bf1ef49bda6620c3cb1635bfc",
      "fce54ab02c844e83ad7228b5fb13c9a9",
      "c7f189e69c41487d8128f431a31f8176",
      "7756405d0c114a479d87c20cba6d6b0d",
      "e95342f4dd8a449385766e008819faf0",
      "516542303e854da2ad0a03d1017245ff",
      "aba6cba6372b420b888ba00a2135869d",
      "b72e25574c7f4c988bcf3e09b3a8882e",
      "d9ec27228d5145a3ad1b30370f57a9dc",
      "19bf972b752541cfbb4732483e8488c6",
      "e40802c6f8a344a6afbdf770c2c6f333",
      "6569105cc1984ce5bee8187b25d9c4f9",
      "cd5805bbce6d4fac820b1c3a98d7acd9",
      "ab761794d8a1429d8af4c7cf40537079",
      "14a76f7e77aa4f74be4dda8ca34e990d",
      "dafebcc64f5b4476a58706c8c0546a8c",
      "c2fa7accefba4be69848091b3e0cfa48",
      "47c5eb2c472e4a70a12273204d2770f8",
      "a33a925f36ef42b88be679d029e5daee",
      "eb796b1209244043ba224eb72ba4c0dc",
      "1b9ba0b385fb403d9ddca3fc5a878a7d",
      "e5d65798bc514ab0958a9e4c6f9440e5",
      "d68486e8185847328671b670bebbe5cb",
      "018d5bc08866419caa222d7576c0b40e",
      "22bb5025519546fe887a0ced405e2419",
      "c67e1e9f51f74e8085d657538c748ce3",
      "8e657a099c55454abd7980db973c8576",
      "08181a07e61c4bc482ec5bc45863b95c",
      "5deede5e681e47469eef3b8147e78910",
      "c2240846d79c4983ba34af747923c1b2",
      "62add7ce4e61492b8d6fb3cb5ef6a66d",
      "16e60050d1bb472abdb210d0b678c0ed",
      "d9dca9aefe2c4ea3b3a0428b82265fc3",
      "1f307c6b63924ee78e1a52cff1c01ae5",
      "2e110e8344c54584bb4a16b773640eea",
      "f55ba9e29dc04fc1b808e05b5b1c0bbc"
     ]
    },
    "id": "s8nuT0EXEpb6",
    "outputId": "6fdeb85c-0089-4785-9af3-7201be669edc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.2%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102.8%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST/raw/train-labels-idx1-ubyte.gz to MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "78.9%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112.7%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST/raw/t10k-labels-idx1-ubyte.gz to MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Pass in transformers to the data here\n",
    "train = datasets.MNIST(\"\", train=True, download=True,\n",
    "                       transform = transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "test = datasets.MNIST(\"\", train=False, download=True,\n",
    "                       transform = transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6ee1205-face-4b53-a89a-9975f278549a",
   "metadata": {
    "id": "KVyb3jNnKmwY"
   },
   "outputs": [],
   "source": [
    "# Using batches of data is more efficient, especially when the dataset\n",
    "# is huge. Also, passing in batches lets the machine learn more generalizations\n",
    "trainset = torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)\n",
    "testset = torch.utils.data.DataLoader(test, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76f747c1-548f-4e3f-8fba-e9c96c45beaf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sz42WEIuMsOO",
    "outputId": "d51b89cf-431e-4e1e-c7ca-06e74d14493c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([8, 5, 8, 6, 3, 6, 9, 6, 9, 3])]\n"
     ]
    }
   ],
   "source": [
    "for data in trainset:\n",
    "  print(data)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6749c51-a4b1-4395-b2b7-c0b130239627",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9_ZD49lBNEdn",
    "outputId": "d1cc88b8-3efb-4ea3-c695-45e3c4c61092"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8)\n"
     ]
    }
   ],
   "source": [
    "x, y = data[0][0], data[1][0]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b79da5f-c983-40cd-84de-8f78eb47f24a",
   "metadata": {
    "id": "9LYxFt9aNdWs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.5.0-cp37-cp37m-macosx_10_9_x86_64.whl (7.3 MB)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Using cached kiwisolver-1.3.2-cp37-cp37m-macosx_10_9_x86_64.whl (61 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from matplotlib) (20.9)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.28.2-py3-none-any.whl (880 kB)\n",
      "\u001b[K     |████████████████████████████████| 880 kB 2.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.2.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from matplotlib) (2.4.7)\n",
      "Collecting setuptools-scm>=4\n",
      "  Using cached setuptools_scm-6.3.2-py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from matplotlib) (1.21.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from matplotlib) (2.8.1)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from matplotlib) (8.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib) (1.14.0)\n",
      "Collecting tomli>=1.0.0\n",
      "  Using cached tomli-1.2.2-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from setuptools-scm>=4->matplotlib) (41.2.0)\n",
      "Installing collected packages: tomli, setuptools-scm, kiwisolver, fonttools, cycler, matplotlib\n",
      "Successfully installed cycler-0.11.0 fonttools-4.28.2 kiwisolver-1.3.2 matplotlib-3.5.0 setuptools-scm-6.3.2 tomli-1.2.2\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.7/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51bba036-3dc4-4786-a885-58b126cdae00",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "TLOn3zUUNg7v",
    "outputId": "758148e2-ce30-4453-b65e-da0d9301dc45"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PyTorch has a different shaping standard. There's a 1x28x28 shaping\n",
    "# style for an image.\n",
    "\n",
    "plt.imshow(x.view([28, 28]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab2e33d8-7785-4043-bc79-68266dee5ed9",
   "metadata": {
    "id": "VZgYp1xiOX_g"
   },
   "outputs": [],
   "source": [
    "# Optimizer tries to decrease loss and it doesn't have any idea\n",
    "# how much better we could get. You want to balance the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8bc10d6-b4b4-4856-82e5-1d1cfa51a660",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xEvNdJe5O4zN",
    "outputId": "166001b4-aab8-4cc7-a45d-54d4bb1db25c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 5923, 1: 6742, 2: 5958, 3: 6131, 4: 5842, 5: 5421, 6: 5918, 7: 6265, 8: 5851, 9: 5949}\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "counter_dict = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0, 9:0}\n",
    "for data in trainset:\n",
    "  xs, ys = data\n",
    "  \n",
    "  for y in ys:\n",
    "    counter_dict[int(y)] += 1\n",
    "    total += 1\n",
    "\n",
    "print(counter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62c233b7-d635-4145-af1b-bcdcaec6f7ad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "piAEpv1_Pprl",
    "outputId": "fc84e926-bc77-4760-bdb4-c8cf6ed03e6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 9.871666666666666\n",
      "1: 11.236666666666666\n",
      "2: 9.93\n",
      "3: 10.218333333333334\n",
      "4: 9.736666666666666\n",
      "5: 9.035\n",
      "6: 9.863333333333333\n",
      "7: 10.441666666666666\n",
      "8: 9.751666666666667\n",
      "9: 9.915000000000001\n"
     ]
    }
   ],
   "source": [
    "for i in counter_dict:\n",
    "  print(f\"{i}: {counter_dict[i]/total*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30a26610-895e-4ced-844a-0710b9501442",
   "metadata": {
    "id": "w4wmtz98QNLu"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "45d9c952-363e-45b5-893c-ae5313d5a7ef",
   "metadata": {
    "id": "_6S2Qx5cQb8a"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    # 3 layers of 64 neurons for our hidden layer\n",
    "    self.fc1 = nn.Linear(784, 64) # Activation function runs on the output side\n",
    "    self.fc2 = nn.Linear(64, 64)\n",
    "    self.fc3 = nn.Linear(64, 64)\n",
    "    self.fc4 = nn.Linear(64, 10)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # Feed forward neural net\n",
    "    # The activation function is whether or not the neuron is firing\n",
    "    # Don't wanna run relu on the output layer. Log softmax is a good choice.\n",
    "\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = F.relu(self.fc2(x))\n",
    "    x = F.relu(self.fc3(x))\n",
    "    x = self.fc4(x)\n",
    "\n",
    "    # Which dimension to apply softmax on? Output is flat, so dim=1\n",
    "    # It is similar to axes.\n",
    "    return F.log_softmax(x, dim=1)\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0ace3ba-5ce1-4ed5-adfb-894bb4118531",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qhPkuFWzkOg5",
    "outputId": "05ef0dd7-63cd-4f1a-bf21-1fedb77d9471"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.3616, -2.2694, -2.3541, -2.1959, -2.2857, -2.2462, -2.4061, -2.3712,\n",
      "         -2.3307, -2.2269]], grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand((28, 28))\n",
    "# Why is the -1 here? Says that the input is of unknown shape\n",
    "X = X.view(-1, 28*28)\n",
    "output = net(X)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27736b28-2b70-426a-a7b0-f38ecb5021bc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "93qxQA4RdpOG",
    "outputId": "f8f00230-1e25-415b-e610-6c4b5c04bb9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1545, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3452, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0661, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Loss and Optimizer. We want to minimize loss obviously. \n",
    "# Note - You can tell your neural network which weights to adjust\n",
    "import torch.optim as optim\n",
    "\n",
    "# You don't want your learning rate to be too large as it won't\n",
    "# catch the nuances of the data. Also, it shouldn't be too small\n",
    "# as the solution will only be a local maxima/minima. \n",
    "\n",
    "# On complex problems, we use a decaying learning rate.\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "EPOCHS = 3 # 3 full passes through data\n",
    "\n",
    "# Zero gradient is if you wanna batch the data up\n",
    "# 2 main ways of calculating loss: one hot vector ([0, 1, 0, ...]) \n",
    "# and nll (for scalar data)\n",
    "for epoch in range(EPOCHS):\n",
    "  for data in trainset:\n",
    "    X, y = data\n",
    "    net.zero_grad()\n",
    "    output = net(X.view(-1, 28*28))\n",
    "    loss = F.nll_loss(output, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "  print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c493adc3-f4f6-4f19-ba75-85ac2e3cd840",
   "metadata": {
    "id": "h_mOBrKuiQjz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "  for data in trainset:\n",
    "    X, y = data\n",
    "    output = net(X.view(-1, 28*28))\n",
    "    for idx, i in enumerate(output):\n",
    "      if torch.argmax(i) == y[idx]:\n",
    "        correct += 1\n",
    "      \n",
    "      total += 1\n",
    "\n",
    "print(\"Accuracy: {}\".format(round(correct/total, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "128861b5-cee1-407e-a77f-57533195ea7c",
   "metadata": {
    "id": "I6dHXqrgDyJY"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f21939b2-4782-427b-995d-3163b8692b66",
   "metadata": {
    "id": "EeqRmKZ3Dzz1"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7)\n"
     ]
    }
   ],
   "source": [
    "plt.imshow(X[1].view(28, 28))\n",
    "plt.show()\n",
    "print(torch.argmax(net(X[1].view(-1, 28*28))[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3a4f6f-408d-4e18-902e-8d3195814d70",
   "metadata": {
    "id": "GxAB0FKjE0LN"
   },
   "source": [
    "# CNN (Convo Neural Net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cdd516be-f0b4-412f-856b-722c5f202743",
   "metadata": {
    "id": "Jfk83ZuYE8Sc"
   },
   "outputs": [],
   "source": [
    "# Traditionally, CNNs were used for image stuff. However, in recent\n",
    "# year, they have even beat RNNs (Reccurent) in sequential problems.\n",
    "# Images are arrays of pixels. Each convolution (or kernel) takes a\n",
    "# subset of these pixels and look for features within them.\n",
    "\n",
    "# Condense the image and then pool them (maxpooling). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b817ea4c-c952-43f6-b4b8-96a0088d15ab",
   "metadata": {
    "id": "OuTrfb80NlEa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (1.21.4)\n",
      "Requirement already satisfied: opencv-python in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (4.5.4.60)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n",
      "\u001b[K     |████████████████████████████████| 76 kB 5.3 MB/s eta 0:00:011\n",
      "\u001b[?25hInstalling collected packages: tqdm\n",
      "Successfully installed tqdm-4.62.3\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.7/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install numpy opencv-python tqdm\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm # progress bar        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7ea11ffe-2035-408d-8345-7fac85c3c94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "REBUILD_DATA = False\n",
    "\n",
    "class DogsVSCats:\n",
    "  # Images have varying size and shape, but we want them\n",
    "  # to be uniform.\n",
    "  IMG_SIZE = 50\n",
    "  CATS = \"catsVSdogs/PetImages/Cat\"\n",
    "  DOGS = \"catsVSdogs/PetImages/Dog\"\n",
    "  LABELS = {CATS: 0, DOGS: 1}\n",
    "\n",
    "  training_data = []\n",
    "  # MAKE SURE THE DATASET IS BALANCED. If there are 10000 pictures of cats and 100 of dogs, you're not\n",
    "  # gonna make a decent neural network ovviamente.\n",
    "  catcount = 0\n",
    "  dogcount = 0\n",
    " \n",
    "    # Everything is a feature to the machine, even the colors of\n",
    "    # an image. So think about whether color makes a difference\n",
    "    # to your neural net.\n",
    "  def make_training_data(self):\n",
    "    for label in self.LABELS:\n",
    "      for f in tqdm(os.listdir(label)):\n",
    "        try:\n",
    "            path = os.path.join(label, f)\n",
    "            img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "            img = cv2.resize(img, (self.IMG_SIZE, self.IMG_SIZE))\n",
    "\n",
    "            # One hot vectors allow a better loss metric. Once again,\n",
    "            # an example of a one hot vector would be [1, 0] if it's\n",
    "            # a cat and [0, 1] if it's a dog. The np.eye function\n",
    "            # helps doing that with the diagonals.\n",
    "            self.training_data.append([np.array(img), np.eye(2)[self.LABELS[label]]])\n",
    "\n",
    "            if label == self.CATS:\n",
    "                self.catcount += 1\n",
    "\n",
    "            elif label == self.DOGS:\n",
    "                self.dogcount += 1\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        \n",
    "    np.random.shuffle(self.training_data)\n",
    "    np.save(\"training_data.npy\", self.training_data)\n",
    "    print(\"Cats: {}\".format(self.catcount))\n",
    "    print(\"Dogs: {}\".format(self.dogcount))\n",
    "    \n",
    "\n",
    "if REBUILD_DATA:\n",
    "    dogvcats = DogsVSCats()\n",
    "    dogvcats.make_training_data()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "867ec211-d1b5-4d3d-96d0-592821e235f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = np.load(\"training_data.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7c62a48f-b17d-4adf-aa5a-dbc72071b505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[143, 219, 215, ...,  46,  12,  28],\n",
      "        [138, 212, 211, ...,  65,  15,  36],\n",
      "        [133, 202, 196, ...,  45,  27,  70],\n",
      "        ...,\n",
      "        [230, 229, 226, ..., 100, 155, 128],\n",
      "        [227, 224, 226, ...,  65, 153, 173],\n",
      "        [227, 227, 226, ...,  67, 136, 183]], dtype=uint8) array([0., 1.])]\n"
     ]
    }
   ],
   "source": [
    "# This is the power of appending the np array of the image and its\n",
    "# associated one hot vector. It'll show what each image is as\n",
    "# numbers; easy for the neural net to digest.\n",
    "print(training_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "80918d20-ec61-4d03-b230-db915891d034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(training_data[1][0], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b5dd8f60-6ceb-4d8b-8b37-2de5e58bcf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a neural network to classify images\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 5 x 5 kernel\n",
    "        # Images are 2D so that's all we need. If you want\n",
    "        # more dimensions, you can code them yourself as well!\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 32, 5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 5)      \n",
    "        \n",
    "        # Now we have to flatten this 2D conv layer to 1D or \n",
    "        # \"flatten\" in TensorFlow terminology\n",
    "        x = torch.randn(50, 50).view(-1, 1, 50, 50)\n",
    "        self._to_linear = None\n",
    "        self.convs(x)\n",
    "        \n",
    "        # The important part is figuring out what the input number\n",
    "        # to the first linear layer is going to be.\n",
    "        self.fc1 = nn.Linear(self._to_linear, 512)\n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "        \n",
    "    # We're gonna run the forward method through the three conv\n",
    "    # layers in an attemp to see the result - to see what to\n",
    "    # pass as input to the input fc1 layer\n",
    "    def convs(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), (2, 2))\n",
    "        if not self._to_linear:\n",
    "            self._to_linear = x[0].shape[0]*x[0].shape[1]*x[0].shape[2]\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Gotta run convs or Flatten twice to know the number of\n",
    "        # input neurons\n",
    "        x = self.convs(x)\n",
    "        x = x.view(-1, self._to_linear)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return F.softmax(x, dim=1)\n",
    "        \n",
    "        \n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fc837832-3cb2-47fa-a42d-2c6ec5707d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "X = torch.Tensor([i[0] for i in training_data]).view(-1, 50, 50)\n",
    "X = X / 255.0    # Scaling/Normalizing the image pixel data\n",
    "# No need to reshape the y (label) values\n",
    "y = torch.Tensor([i[1] for i in training_data])\n",
    "\n",
    "# Seperating training/validation data\n",
    "VAL_PCT = 0.1\n",
    "val_size = int(len(X)*VAL_PCT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d6d1f79e-9dc4-40a3-8f4a-02330f363bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = X[:-val_size]\n",
    "train_y = y[:-val_size]\n",
    "\n",
    "test_x = X[-val_size:]\n",
    "test_y = y[-val_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a4ec533a-c387-4901-9955-b116e6f68815",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 225/225 [01:02<00:00,  3.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2490, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 100\n",
    "EPOCHS = 1\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for i in tqdm(range(0, len(train_x), BATCH_SIZE)):\n",
    "        batch_x = train_x[i:i + BATCH_SIZE].view(-1, 1, 50, 50)\n",
    "        batch_y = train_y[i:i + BATCH_SIZE]\n",
    "        \n",
    "        # gotta zero the gradients\n",
    "        # no difference between optimizer.zero_grad and net.zero_grad\n",
    "        # if you've passed all the network's parameters inside the optimizer.\n",
    "        \n",
    "        net.zero_grad()\n",
    "        outputs = net(batch_x)\n",
    "        loss = loss_function(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9044d63b-15d9-4f1b-8e61-f33420b949e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 2494/2494 [00:03<00:00, 688.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(len(test_x))):\n",
    "        real_class = torch.argmax(test_y[i])\n",
    "        net_out = net(test_x[i].view(-1, 1, 50, 50))[0]\n",
    "        predicted_class = torch.argmax(net_out)\n",
    "        \n",
    "        if predicted_class == real_class:\n",
    "            correct += 1\n",
    "        \n",
    "        total += 1\n",
    "        \n",
    "print(\"Accuracy: {}\".format(round(correct/total, 3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bd67d9-b0b9-4232-810f-2958d3155812",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
