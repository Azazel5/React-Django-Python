Introduction
------------

Machine learning is great for complex, real world problems where the situation is dynamic and ever changing.

Some example applications of machine learning:

* Image classification is traditionally done via CNNs (Convolutional Neural Networks)
* Semantic segmentation -> labelling each pixel of an image with a class of what is being represented. Used
in detecting tumors in brain scans via CNNs
* Text classification and NLP, done via RNNs (Recurrent Neural Networks), CNNs, or Transformers
* Anomaly detection - example, credit card fraud
* Regression, SVMs (Support Vector Machines), Random Forests
* Clustering, segregating classes of data

! Types of machine learning systems

1. Supervised, Unsupervised, Semisupervised, and Reinforcement learning

* Feature extraction is a technique used in dimensionality reduction AKA when the data is pretty complex and
you want to simplify it as much as possible without losing any information. Example, if we find that a human's
interests is strongly dependent on age and gender, we could combine these two feature into one bigger feature.

An example of semisupervised learning algorithm: DBNs (Deep Belief Networks) are based on RBMs (Restricted 
Boltzmann Machines)

Reinforcement learning is based around rewards and policies. Each action that an agent takes has a cost, 
which may be positive or negative. The goal of the agent is to maximize reward and minimize penalties,
which it does via the policy which dictates what action it should take in which situation.


2. Batch and Online learning 

Batch learning systems learn everything at once and is incapable of learning from a stream of incoming data 
making it inflexible. Online learning is fed mini-batches of data and can adapt to new incoming data nicely.

* An important parameters for online learning systems is the learning rate. 
High rate = may forget old data more quickly but adapts to new data well
Slow rate = more inertia but less sensitive to outliers and noise

* Monitoring is important in online learning systems because bad data means a decline in the system's 
performance. There should be options to switch off learning, revert to an earlier stage, or react to abnormal
data using an anomaly detection algorithm for instance. 

3. Instance Based versus Model Based 

Instance based learning is a trivial kind of generalization where it learns the training data by heart and 
tests the new data by the similarity in any feature; for example, it could say that an email is spam if it 
contains a similar number of words as it saw in spam emails in its training dataset. 

On the other hand, we could select a model with a set of attributes to make generalizations. Here the algo
selects parameters and evaluates them using a fitness function. 

* For regression, people use cost functions which measure how far off your model's prediction is as compared 
to the training data. The goal is to minimize the cost function's output. 

* Training a model means finding the best set of parameters which maximizes fitness function, so it can be 
used with fresh, real world data for prediction. 

* Major problems in machine learning include insufficiency of data/bad data or choosing a bad algorithm.

* Ways to reduce overfitting is to select a model with fewer parameters, select fewer attributes, or get more
data.

* A hyperparameter is a parameter of the learning algorithm itself and not the model.

* Ways to reduce underfitting is, as you might've guessed, opposite. 

! Testing and Validating

Split your dataset into a training and testing set. Calculating the error rate is important!
If training error is low and testing error is high = overfitting

* It's common to go for a 80:20 (training testing) split, but depending on how large the dataset is, you can
change the ratio. What if you're deciding in between several models and want to select the one which does the
best? The solution is the validation set, purely to evaluate models. You can train several models with 
differing hyperparameter values to see which performs the best on the set. The best one is selected and is 
then trained on the full training set (including the validation set). 

* The testing and validation sets must be representative of the data used in production! 

End-To-End Machine Learning Project 
-----------------------------------

The first order of business calls for analyzing the problem: what exactly are we trying to solve? Looking at 
the problem, it is clearly supervised, batch, and model based. This is a multiple regression problem, as we'll
use several attributes in the data to make a decision. 

Now we must define a performance measure (fitness function?). For regression, this can be a RMSE (Root Mean 
Square Error). 

* os.makedirs will make directories recursively even if they don't exist. If directory already exists, you 
can specify exist_ok=True to not raise an OS exception. 

Making a function which either downloads the data or scrapes it regularly is a pretty useful thing to create
for an ML project. 

* df.info() and df.head() are useful!

Some attributes will have lesser number of values, which you need to take care of before training your model.
You can find categories of certain series by pd["column_name"].value_counts() function.

* There's also the describe function which gives other statistical information such as the mean, median, etc.

Gotta understand how the data has been preprocessed. For instance, the median income was capped in the data 
provided by the book. This is not a huge problem, but the same is not true for the capping of median housing
value, which is what the algorithm is trying to predict. 

* Having tail heavy or head heavy data is bad for ML algorithms; it is preferred to have a bell shaped graph.

It is preferred to put away data for the test set right now! Splitting the test train set should be done 
carefully so as to have the same test data even if the dataset is updated or the function is run again. 
This is precisely why people use np.random.seed(42), to generate the same random numbers as before.

* A good solution is to compute the hash value of each instance and put it in the test set if its value is
less than or equal to 0.2 * maximum hash value. This ensures that the same data will be in the test set AND
that the test set is 20% of the total set. 

* The zlib crc32 function can be used to generate a checksum identifier for any piece of data.

There are boolean operators available for pandas dataframes, such as data.loc[in_test] or data.loc[~in_test].
Since the in_test_set returns a subset of the id columns which satisfied the test_check function, you can get 
all columns which satisfy those requirements.

If there isn't a unique identifier in the data, just use the row number (gotta make sure new data gets 
appended to the end and no row gets deleted). If this isn't possible, make your own unique id column, 
such as id = latitude + 1000 + longitude.

* Random sampling is good when the dataset is large enough; however, if that's not the case, we run into the 
risk of creating sampling bias.

Stratified sampling is a technique used to maintain the known ratio of something in the sample. 

* The pd.cut() function can be used to represent a continuous variable in a categorical format. Example -
the median salary - 5 distinct categories.

Scikit-learn has many functions for splitting data into training/testing sets, so make sure to use that 
instead. Here, we'll use the stratified split sampling function from the veteran library. Test sets 
generated using pure random sampling will be skewed, so moral of the story: use stratified sampling.

[!] Visualizing the data 

After splitting the dataset into a test and train set, it is important to start visualizing the data. In 
really big datasets, you may want to sample an exploration set which is small for quicker manipulations.
Setting alpha=0.1 makes it easy to see the high density data points. You can specify a cmap (color map) to
make things look better. 

* Here the correlation coefficient comes in handy, especially because the dataset isn't too large. When 
the correlation coefficient is close to 1, that means there's a strong correlation there. The corr coef 
only finds linear relationships and completely misses out on others.

The most crucial thing that visualizing data reveals is that there may be some things that you want to
fix before proceeding with the ML algorithm. Also, you discover some associations which may come in handy.
If tail heavy action happens, compute the logarithm. Also try combining attributes together. The example 
given in the book is crisp: total number of rooms isn't very useful but number of rooms per household is (if
you know enough to calculate that). 

This doesn't need to be extremely thorough! It's just meant to get you started. Make sure to seperate 
predictors and labels, since we're doing supervised ML right now. What we're trying to predict is median
house value, so we will capture that in another variable.

* The function df.drop returns a copy of the dataframe with the dropped row/column.

[!] Cleaning the data 

Here's comes the most boring (yet important so they say) part of ML! Here Scikit-learn shines again,
with an awesome class which handles missing values AKA SimpleImputer. Say you pass the median strategy
to the SimpleImputer class. Now you won't be able to use the ocean_proximity attribute anymore because it
is categorical, so you have to make sure to remove that. The imputer replaces missing values with the 
learned medians. 

* Most machine learning algorithms prefer to work with numbers. So we can use another Scikit-learn function
to encode the text to numbers. Also, one thing to understand is that how you put together categories matters.
For example, an ML algorithm may assume that "two nearby values are more similar than two distant values",
which may not be true. The fix is to go for "One-Hot Encoding" i.e. 0 or 1 for each category.

After one-hot encoding, we get back a sparse matrix (with a lotta zeroes) because there could be thousands 
of categories. 

* If the number of categories for the attribute is too large (for example, a student's major), you could 
represent this categorical information using a different attribute or a learnable, low-dimensional vector -
this process is called embedding (its representation is learned during training AKA representational
training). OOF, all this terminology.

* The numpy.c_ function concatates objects along an axis. For instance, we got [1, 2, 3] and [4, 5, 6],
np._c[arr1, arr2] ([ brackets!) gives 
[
    [1, 4],
    [2, 5],
    [3, 6]
]

Scikit-learn also makes it pretty simple to write your own classes, whether it be a transformer or whatever.
The book proceeds to create a hyperparameter using a custom transformer class i.e. one which adds 
bedrooms_per_household as a column values depending on the constructor. 

* Machine learning algorithms don't perform well when the inputs have different scales, so feature scaling 
must be performed. Two common ways to do this: min-max (normalization) scaling and standardization.

- Normalization: make sure data ranges from 0 to 1. Subtract minimum value and divide by max - min AKA
min / (max - min). Scikit-learn -> MinMaxScaler

- Standardization: subtract the mean value and divide by standard deviation. This process does not produce 
a range which might be a problem for neural networks, but is much less affected by outliers obviously.
Scikit-learn -> StandardScaler

Scikit also provides a pipeline class which can take in all these objects in a particular order too. You
can add in everything from the imputer discussed above to the custom transformers you create.
This approach led us to handle categorical and numerical columns seperately, but Scikit has a 
ColumnTransformer which can handle all things at once. 

* When giving the ColumnTransformer values, you can specify "drop" or "passthrough" if you want to drop 
certain columns or want them to be left untouched. 

Calculating the root mean squared error is a good idea for linear models. Naturally, Scikit has a function
for that. The calculated RMSE value shows that we're underfitting heavily right now. So, let's pick a more
powerful model i.e. the DecisionTreeRegulator. Using this, we get an error of 0, which isn't right.

Clearly, we go through the case of selecting different models in the case of underfitting and overfitting. 
This is why the validation set is important, which validates a model. We want to train the model which 
performs the best on the validation set, which is finally tested with the test set. What a process. If 
this is too much work, just use Sklearn's K-fold cross validation!! This function takes in the model,
data, labels, scoring, and k (the number of folds). Using this, we can see that the Decision Tree is 
actually worse than the Linear Regression.

Let's try one more model: the RandomForestRegressor. This trains many Decision Trees based on random
attributes and averages them out. Although this is better, it still shows overfitting.

* The goal is to shortlist 2-5 best models. Save and load models using the Pickle or Joblib libraries.

Now that you have selected some promising models, the goal is to make them as good as possible. This involves
playing with parameters which is super boring. Sklearn to the rescue again with GridSearchCV. This search 
involves cross validation too (hence the CV). You have to pass in a param grid with number esitmators, 
max values, etc. 

The book gives this example of such a param grid: param_grid = [
   {'n_estimators': [3, 10, 30],
    'max_features': [2, 4, 6, 8]},
    {'bootstrap': [False],
     'n_estimators': [3, 10],
     'max_features': [2, 3, 4]}           
]

Total grid search will be 3 * 4 + 2 * 3 = 18
Since we're using 5 fold CV, it'll be 18 * 5 = 90 training sessions
This will give a lot of information through the best_params_ or best_estimator_ attributes. 

What about when there are way too many features in the data to do this for? RandomizedSearchCV. 
GridSearch can also demonstrate the importance of each feature's importance in making decisions.

After this the book dives into more detail of parameter fine tuning, but the moral of the story is:
drop the parameters which don't affect the dataset too much. Now the time is to test the final model 
with the test set. 95% confidence interval for generalization error is another idea as compared to
the root mean square method.

* Even simplistic models like ones which classify cats and dogs may need to be retrained over time
because there are so many factors outside our grasp in the real world i.e. cameras may change, people
may start dressing up dogs with hats and puss in boots. 

Classification
--------------

The dataset has 70000 images with 784 features each because each image is 28 * 28 = 784.

* Matplotlib can also show images, using the imshow function. However, these images may need to be 
reshaped; for instance, I had to reshape a MNIST dataset instance to a 28 x 28 array. A binary classifier
can detect a 0 or 1 or, in other words, instances of a class k and instances of a class not(k). 

* The stochastic gradient descent class of Sklearn (SGDClassifier) is a good class for large datasets and
for online learning. Set the random_state attribute! 

Evaluating a classifier is much more tricky business than a regressor. One instance which demonstrates this 
well is that the classifier we built for just detecting non-5 numbers has a 90% accuracy. This is because 
encountering a number 5 has a base probability of about 0.1, so if the classifier just guesses not-a-5 every
time it will have a 90% accuracy. Not what we want at all...

A better way is to draw the confusion matrix i.e. number of times instances of class A was classified as class
B. Each row in the matrix is an actual class and each column is a predicted class. Therefore, a perfect
classifier has zeroes along its opposite diagonal or only true positives and true negatives. 

* Precision, accuracy of positive predictions, must be used with a concept called recall because a classifier
with just 1 true positive may be labelled as precise.

It may be useful to create a metric which combines both precision and recall, often called the F1 score, the
harmonic mean.

* Harmonic mean takes the weight of lower weight values highly as compared to the normal mean. A classifier
only does well if both precision and recall are high. 

As always in machine learning, there is no rule which applies to all scenarios. You don't always want both 
high precision and recall! Precision and recall have an inverse relationship. The SGDClassifier uses a 
threshold value of 0 to determine what precision and recall it's going to get. However, we've seen that 
a different threshold value affects these scores. What value to use? 

* Pass "decision_function" instead of accuracy to cross_val_predict to get the decision scores. Now you can
pass these values onto the precision_recall_curve to see what's up. You can search which threshold value 
corresponds to a precision higher than 0.9 for instance, using np.argmax.

* A high precision classifier isn't too useful if its recall is abysmal.

The ROC curve - Receiver Opening Characteristic - plots the true positive rate against false positive rate.
A good classifier has a normal ROC curve and it isn't edged out in any one area.

* A perfect classifier has ROC area under curve = 1 and random classifier = 0.5 

Multiclass Classification
-------------------------

Multiple class classification can be done in many ways. For instance, you can train a binary classifier
for each digits 1 to 10, and get the decision score from each classifier, and pick the highest one. There
are also other ways this can be done, but the method described in generally preferred. 

* The SVC class determines whether to use One-V-One or One-V-Rest by itself, but this too can be selected
by using the appropriate classes from Sklearn.

* Use Matplotlib's matshow function to see a nice visualization of the confusion matrix.

After finding the right model to use, hypertuning its parameters, and calculating accuracy, we gotta do
cross validation and other techniques described for error analysis. In the example used in the book, while
using SGD, we noticed that the model was quite sensitive in differentiating 3's and 5's. Preprocessing images
to make sure they're straight is a great tactic.