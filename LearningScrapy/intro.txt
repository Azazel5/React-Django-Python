HTML and XPATH 
--------------

When a person goes to a website, what happens? First of all, the domain name is used to reach the server on
which the website is hosted (uses the DNS to do this). The server returns HTML, JSON, XML, or other responses; if it is HTML, it 
gets drawn on the browser via the Document Object Model (DOM). The HTML you write is texual, but the 
DOM view you see in most major browsers are objects loaded into its memory, which can be manipulated.

Xpath is a great selector to use because we can directly select elements from the DOM, according to 
hierarchy. In chrome developer tools, the $x function can be used to select from xpath. Example -
$x('//h1'), the h1 element (this function returns an array). The // syntax can be used to find elements
no matter where they are. To find any links in divs, we can use the '//div//a' xpath. Selecting via
attribute - '//a[@href]', text(), or all elements by * - '//p/*'. 

There are functions like starts-with, ends-with, contains, etc to select elements with attributes. To
check out a website with the scrapy shell, do scrapy shell https://website.com. Through this, we have 
access to the response object, which can be used to analyze the website. You can even combine xpath 
functions with boolean conditions like "and". 

Basic Crawling
--------------

Remember the UR^2IM method: urls, request, response, items, and more url. Debug problems with the shell 
using the --pdb argument. If you simply select elements containing text such as <p> or <h1> tags, it is 
a good practice to add the text() property at the end of the xpath to avoid extracting the HTML text 
itself. 

Reminder: Xpath starts indexing from 1!

You can chain the xpath and css functions together if you wish. The fact that we declare a field in
items.py isn't a guarantee that we're gonna find/use it in each spider or at all. The author also 
recommends adding a couple housekeeping fields which are helpful, espcially while debugging. After 
defining the items, we can move on to the spiders directory, where you can create all the spiders in
the project. You can also save your output into any file formats by specifying the -o file.ext
parameter. Also, scrapy can save the outputs to someplace other than your computer using the -o
file_path_and_ext argument. 

Processors are basically higher order functions which you can use to manipulate the results you get 
back from the website. Important ones include: Join, MapCompose, etc. You can hardcode which links 
to pass to the spider and also open a file, read its lines, and start scraping; however, the most 
exciting thing is that you can make scrapy use the website's pagination system to move forward too. 

Yield is a great keyword to know about! In all your years of coding, you've seen many instances where
you had to define a list and then append stuff to it in a for loop. This could easily be done 
with a yield, which is like return, but it doesn't cause the code to exit out the function/script. 
You can add an argument -s CLOSESPIDER_ITEMCOUNT=90 to limit the number of items scraped. If you wanna
send out multiple requests, accumulate them all using the yield generator and the scrapy.http Request 
object and pass it into a helper parsing method; this working because the parsing method takes in a 
single response object, which'll be called sequentially.

From Scrapy to a mobile app 
---------------------------

Through Scrapy we can also easily specify where we want to store data; for example, if we have a 
deployed database url on the cloud, we can specify its database column names, such as how we did 
it with Appery.io in the book. 

Quick spider recipes
--------------------

This chapter, although very interesting, won't be followed by me because I didn't set up the virtual 
machine in the beginning, so I cannot access the web:/dynamic link. Instead, I'll just read through 
it and write down interesting aspects here. After a successful post request with valid usernames and 
passwords, the server usually response with a 302 found status code and redirects the user agent 
to a new link. Servers only assign cookie values after succesful logins, so you don't receive 
responses on API requests without authentication. The relationship is as follows:
server sends my browser a cookie on a login -> on each subsequent request the browser sends it back
to identify us 

Instead of simple start_urls, we can also choose to have a more complex start_requests function. 
Scrapy is awesome at passing cookies around; I assume, under the hood, it's doing something similar 
like "with requests.Session()" or something like that. After a login, we can simply copy the rules 
and parsing method we had in our other spiders. Some websites also have hidden input fields assigned 
on login, which is needed for the post request to be successful. To handle this, we can return a 
Request in the start_urls function and handle it using the FormRequest's from_response function.

Now, we finally arrive at the long awaited portion of this book. How do we handle websites using 
APIs and AJAX using scrapy? APIs are pretty straightforward, as you use the json loads function,
parse whatever it is you want, and send further Requests if you so wish. You can also pass 
arguments between responses. You can pass stuff you find from the API as a meta tag and retrive it
in the callback function. Sometimes, websites will have context nodes, which are lists of xpath 
expressions. Using these may reduce the number of requests you send out. The sibling elements 
are selected with a . before each query, as in .//*[@class="blah"]. 

What about a spider that crawls based on an Excel file? This would be useful if you have several 
similar types of websites, which only differ in Xpath. You could create columns which hold the 
xpaths. In the code for this spider, we don't define fields in items.py, but we rather pass them 
to the item_loader manually by creating an item object. In scrapy, you can pass command line 
arguments using the -a argument in this fashion:  scrapy crawl fromcsv -a arg1=val. Accessing of 
values can be done by the getattr python function through which we can also specify default values 
when the argument isn't passed. 

Deploying to scrapinghub 
------------------------

Scrapinghub is a great resource for deploying spiders; there's a free tier which you can use without 
any payment headaches! 

Configuration and Management
----------------------------

Alongside each scrapy project's default settings file, we can also specify settings for each spider. 
Also, we can use the -s command to specify last minute commands, such as when we limited the scraped 
items count to 90 earlier. 

Scrapy also features a telnet console which may be used to look at the running scrapy instance's 
internals. If you have telnet installed, using the command "telnet localhost portno" will open
the above stated instance, where you could run commands like est(). You could even pause crawls
from here! Concurrent requests are the total requests doable depending on how long one request
takes for your system. For example, if one request takes 0.5 seconds and CONCURRENT_REQUESTS = 16,
your request limit is 16 / 0.5 = 32 requests per second. 

There are other ways to stop crawls early, such as -s CLOSESPIDER_ITEMCOUNT, CLOSESPIDER_PAGECOUNT,
or CLOSESPIDER_TIMEOUT. Scrapy also has a caching mechanism disabled by default, which can be enabled
via the HTTPCACHE_ENABLED = True. Also you can specify which databased to store those cached files in.
If you run it with the cache option, scrapy creates a new .scrapy directory to hold all things cache.
Long story short, scrapy gives developers tons of flexibility. 

As we've seen, scrapy utilizes a depth first philosophy, which can be changed to breadth first if you 
so desire. Example - if you're trying to scrape a news website, you might want to scrape top level,
more recent news first, which would ask for a breadth first strategy. 

You can use scrapy feeds if you want whatever you scraped to be sent to a local filesystem or a remote 
webserver. You can specify the FEED_URI or FEED_FORMAT to let scrapy know where the data is going and
what format it's in. 

Using scrapy to download media? Yes, it can be done. Images can be saved anywhere using the methods 
discussed above or you can give it a relative path, so it is saved to the project directory. It is
amazing how many options there are! You could filter out images by their size and also determine
how many days until the images get deleted. You could also download files using the same methodology.
To use image/file pipelines, add it to the settings page. 

Proxies? Easy. First choose a free proxy and check if it is valid by looking through HMA's public proxy
list. Export an environment variable http_proxy=1.1.2.3:9000 and start sending requests through that IP.
There's also a service called Crawlera (made by the people who created scrapy itself) which is worth
looking into. They use network techniques alongside a large pool of IP addresses to hide your scraper:
the best part is that you can use the same environment variable alongisde your Crawlera username and
account i.e. export http_proxy=myusername:mypassword@proxy.crawlera.com:8010

You can customize scrapy settings by using ITEM_PIPELINES or COMMANDS_MODULE. As always, you can extend 
a class, overload some functions, and slap your new custom class onto settings (like you'd do in Django),
but you better know what you're doing in that case. There's a lot of options that scrapy provides, even 
for something as basic as redirects; it is extremely detailed, as you could set redirect priorities or 
max number of redirects. Exploring all these settings and this framework will take time and many projects.

Programming Scrapy 
------------------

Do not, under any circumstances, write code that blocks!!

Twisted, what scrapy is based upon, prefers utilizing a single thread for operations, unlike multithreading.
If there's a blocking operation, such as writing to a database, Twisted returns a hook to it. There's a 
deferred callback for that hook such that, whenever the operation is completed, the hook will be called, 
and there's no reason for any blockage of the main thread. 

This chapter demonstrates that configuring scrapy to suit your own needs is pretty straightforward business:
you could modify your spider, write an item pipeline, write a spider middleware, write a download middleware, 
or an extension. A use case of a pipeline could be - if you spiders provide crawl date in Python format and 
you want to save it as a string, you could write a pipeline to convert them all for you rather then change 
all your spiders' settings. All you need to do is to create a class with a process_item function. After 
this you should set your ITEM_PIPELINES in the settings.

Pipeline Recipes 
----------------

