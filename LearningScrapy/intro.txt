HTML and XPATH 
--------------

When a person goes to a website, what happens? First of all, the domain name is used to reach the server on
which the website is hosted (uses the DNS to do this). The server returns HTML, JSON, XML, or other responses; if it is HTML, it 
gets drawn on the browser via the Document Object Model (DOM). The HTML you write is texual, but the 
DOM view you see in most major browsers are objects loaded into its memory, which can be manipulated.

Xpath is a great selector to use because we can directly select elements from the DOM, according to 
hierarchy. In chrome developer tools, the $x function can be used to select from xpath. Example -
$x('//h1'), the h1 element (this function returns an array). The // syntax can be used to find elements
no matter where they are. To find any links in divs, we can use the '//div//a' xpath. Selecting via
attribute - '//a[@href]', text(), or all elements by * - '//p/*'. 

There are functions like starts-with, ends-with, contains, etc to select elements with attributes. To
check out a website with the scrapy shell, do scrapy shell https://website.com. Through this, we have 
access to the response object, which can be used to analyze the website. You can even combine xpath 
functions with boolean conditions like "and". 

Basic Crawling
--------------

Remember the UR^2IM method: urls, request, response, items, and more url. Debug problems with the shell 
using the --pdb argument. If you simply select elements containing text such as <p> or <h1> tags, it is 
a good practice to add the text() property at the end of the xpath to avoid extracting the HTML text 
itself. 

Reminder: Xpath starts indexing from 1!

You can chain the xpath and css functions together if you wish. The fact that we declare a field in
items.py isn't a guarantee that we're gonna find/use it in each spider or at all. The author also 
recommends adding a couple housekeeping fields which are helpful, espcially while debugging. After 
defining the items, we can move on to the spiders directory, where you can create all the spiders in
the project. You can also save your output into any file formats by specifying the -o file.ext
parameter. Also, scrapy can save the outputs to someplace other than your computer using the -o
file_path_and_ext argument. 

Processors are basically higher order functions which you can use to manipulate the results you get 
back from the website. Important ones include: Join, MapCompose, etc. You can hardcode which links 
to pass to the spider and also open a file, read its lines, and start scraping; however, the most 
exciting thing is that you can make scrapy use the website's pagination system to move forward too. 

Yield is a great keyword to know about! In all your years of coding, you've seen many instances where
you had to define a list and then append stuff to it in a for loop. This could easily be done 
with a yield, which is like return, but it doesn't cause the code to exit out the function/script. 
You can add an argument -s CLOSESPIDER_ITEMCOUNT=90 to limit the number of items scraped. If you wanna
send out multiple requests, accumulate them all using the yield generator and the scrapy.http Request 
object and pass it into a helper parsing method; this working because the parsing method takes in a 
single response object, which'll be called sequentially.

From Scrapy to a mobile app 
---------------------------

Through Scrapy we can also easily specify where we want to store data; for example, if we have a 
deployed database url on the cloud, we can specify its database column names, such as how we did 
it with Appery.io in the book. 

Quick spider recipes
--------------------

This chapter, although very interesting, won't be followed by me because I didn't set up the virtual 
machine in the beginning, so I cannot access the web:/dynamic link. Instead, I'll just read through 
it and write down interesting aspects here. After a successful post request with valid usernames and 
passwords, the server usually response with a 302 found status code and redirects the user agent 
to a new link. Servers only assign cookie values after succesful logins, so you don't receive 
responses on API requests without authentication. The relationship is as follows:
server sends my browser a cookie on a login -> on each subsequent request the browser sends it back
to identify us 

Instead of simple start_urls, we can also choose to have a more complex start_requests function. 
Scrapy is awesome at passing cookies around; I assume, under the hood, it's doing something similar 
like "with requests.Session()" or something like that. After a login, we can simply copy the rules 
and parsing method we had in our other spiders. Some websites also have hidden input fields assigned 
on login, which is needed for the post request to be successful. To handle this, we can return a 
Request in the start_urls function and handle it using the FormRequest's from_response function.

Now, we finally arrive at the long awaited portion of this book. How do we handle websites using 
APIs and AJAX using scrapy? APIs are pretty straightforward, as you use the json loads function,
parse whatever it is you want, and send further Requests if you so wish. You can also pass 
arguments between responses. You can pass stuff you find from the API as a meta tag and retrive it
in the callback function. Sometimes, websites will have context nodes, which are lists of xpath 
expressions. Using these may reduce the number of requests you send out. The sibling elements 
are selected with a . before each query, as in .//*[@class="blah"]. 